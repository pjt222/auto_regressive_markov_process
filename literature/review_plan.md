# Literature Review Plan

## Phase 1: Initial Collection (Current)
1. **Gather Core References**
   - [ ] Visit Eran Barenholtz's faculty page and publication list
   - [ ] Download/bookmark Julian Barbour's key papers
   - [ ] Explore The Gray Cuber website and documentation
   - [ ] Watch Barbour's complexity lecture (take notes on relevant timestamps)

2. **Foundational Reading**
   - [ ] Review fundamental HMM papers
   - [ ] Survey auto-regressive model architectures
   - [ ] Understand Barbour's complexity framework
   - [ ] Study geometric computation approaches

## Phase 2: Deep Dive
1. **Connection Points**
   - [ ] Identify papers that bridge auto-regression and Markov processes
   - [ ] Find work on preserving Markov properties in neural networks
   - [ ] Look for geometric interpretations of embeddings

2. **Mathematical Foundations**
   - [ ] Formalize the connection between auto-regression steps and Markov transitions
   - [ ] Understand complexity measures applicable to embedding spaces
   - [ ] Study n-dimensional shape representations

## Phase 3: Synthesis
1. **Create Review Documents**
   - [ ] Write comprehensive review of each topic area
   - [ ] Identify gaps in current research
   - [ ] Formulate specific research questions

2. **Develop Framework**
   - [ ] Sketch mathematical framework combining all elements
   - [ ] Identify implementation challenges
   - [ ] Plan proof-of-concept experiments

## Tools & Resources

### Paper Management
- Consider using Zotero, Mendeley, or similar for reference management
- Create BibTeX file for formal citations

### Reading Notes
- Use the template in bibliography.md for consistent notes
- Cross-reference between papers
- Track key equations and algorithms

### Visualization
- Create concept maps showing connections
- Diagram the proposed architecture
- Sketch mathematical relationships

## Timeline
- Week 1-2: Initial collection and foundational reading
- Week 3-4: Deep dive into connections
- Week 5-6: Synthesis and framework development

## Key Questions to Address
1. How can auto-regression steps formally map to Markov state transitions?
2. What role does complexity (in Barbour's sense) play in embedding space structure?
3. How can n-dimensional shapes represent computational processes?
4. What ensures Markov property preservation during auto-regressive updates?
5. How do these concepts integrate into a unified framework?