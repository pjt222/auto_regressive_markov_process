# auto regressive markov processes

idea: combine the following:
- auto regression (e.g. llm-reasoning, review https://www.fau.edu/science/directory/ebarenholtz/)
- (hidden) markov processes (each self feed of the autoregressiv process is one markov step. the markov eiogenschaft should not be violated, since the autoregression updates the network)
- embedding/vactor space might be grounded in "complexity" by Julien Barbour and colleagues: https://www.youtube.com/watch?v=q-bImnQ9cmw&t=7426s ; https://en.wikipedia.org/wiki/Julian_Barbour
- also use aproach of https://thegraycuber.github.io/ of calculating with shapes (I think of n-dimensional shape as we have a n-dimensional embedding vector)